# --- step39_news_filter.py (Truly Standalone Version v3.0) ---
"""
Vestra Data Utility Engine - AI æ–°èéæ¿¾èˆ‡è©•åˆ†æ¨¡çµ„
Standalone ç‰ˆæœ¬ï¼šå…§å»º AI å‘¼å« + R2 ä¸‹è¼‰é‚è¼¯ï¼Œä¸ä¾è³´ä»»ä½•å¤–éƒ¨å°ˆæ¡ˆæª”ã€‚
"""

import sqlite3
import os
import json
import requests
import re
import base64
import time
from datetime import datetime
from typing import List, Dict, Tuple, Optional

# R2 ä¸‹è¼‰åŠŸèƒ½éœ€è¦ boto3
try:
    import boto3
    from botocore.client import Config
    BOTO3_AVAILABLE = True
except ImportError:
    BOTO3_AVAILABLE = False
    print("  > [System] æœªå®‰è£ boto3ï¼Œç„¡æ³•å¾ R2 ä¸‹è¼‰æ•¸æ“šã€‚")

# Supabase æ•´åˆ
try:
    from supabase import create_client, Client
    SUPABASE_AVAILABLE = True
except ImportError:
    SUPABASE_AVAILABLE = False

# é…ç½®å€
INVESTMENT_DB = "investment_news.db"
DEFAULT_MODEL = "google/gemini-2.0-flash-exp:free"
MIN_SCORE_THRESHOLD = 5

# ==================== 1. å…§å»º R2 ä¸‹è¼‰åŠŸèƒ½ ====================
def download_from_r2(date_str, db_type='news'):
    """å¾ R2 ä¸‹è¼‰æŒ‡å®šæ—¥æœŸçš„è³‡æ–™åº« (å…§å»ºç‰ˆ)"""
    if not BOTO3_AVAILABLE:
        return None
        
    # å¾ç’°å¢ƒè®Šæ•¸è®€å–é…ç½®
    r2_endpoint = os.environ.get("S3_ENDPOINT_URL")
    r2_key_id = os.environ.get("S3_ACCESS_KEY_ID")
    r2_secret = os.environ.get("S3_SECRET_ACCESS_KEY")
    bucket_name = os.environ.get("S3_BUCKET_NAME", "trendradar-news")
    
    if not (r2_endpoint and r2_key_id and r2_secret):
        print("  > [R2] ç¼ºå°‘ R2 ç’°å¢ƒè®Šæ•¸ (S3_ENDPOINT_URL ç­‰)")
        return None

    try:
        s3 = boto3.client('s3',
            endpoint_url=r2_endpoint,
            aws_access_key_id=r2_key_id,
            aws_secret_access_key=r2_secret,
            config=Config(signature_version='s3v4'),
            region_name='auto'
        )
        
        file_name = f'trendradar_{db_type}_{date_str}.db'
        object_key = f'{db_type}/{date_str}.db'
        local_path = file_name
        
        print(f"  > [R2] æ­£åœ¨ä¸‹è¼‰: {object_key} -> {local_path} ...")
        s3.download_file(bucket_name, object_key, local_path)
        print(f"  > [R2] âœ… ä¸‹è¼‰æˆåŠŸ: {local_path}")
        return local_path
        
    except Exception as e:
        print(f"  > [R2] âš ï¸ ä¸‹è¼‰å¤±æ•— ({object_key}): {e}")
        return None

# ==================== 2. å…§å»º AI æ ¸å¿ƒ (Rate Limited) ====================
def initialize_services():
    """ç‚ºäº†ç›¸å®¹æ€§ä¿ç•™ï¼Œå¯¦éš›ä¸Šä¸éœ€è¦åšå¤ªå¤šäº‹"""
    pass

def call_openrouter(model, messages, temperature=0.3):
    """å…§å»ºç°¡æ˜“ç‰ˆ OpenRouter Caller (å«é‡è©¦æ©Ÿåˆ¶)"""
    key = os.environ.get("OPENROUTER_API_KEY")
    if not key:
        print("  > [AI] âŒ ç¼ºå°‘ OPENROUTER_API_KEY")
        return None
    
    # å®šç¾©é‡è©¦æ¬¡æ•¸
    max_retries = 3
    
    for attempt in range(max_retries):
        try:
            res = requests.post(
                "https://openrouter.ai/api/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {key}",
                    "HTTP-Referer": "http://localhost:8501",
                    "X-Title": "Vestra AI Filter"
                },
                json={"model": model, "messages": messages, "temperature": temperature},
                timeout=60
            )
            
            if res.status_code == 200: 
                return res.json()['choices'][0]['message']['content']
            elif res.status_code == 429:
                # é‡åˆ°é™é€Ÿï¼Œç­‰å¾…å¾Œé‡è©¦ (æ›´åŠ æ¿€é€²çš„é€€é¿)
                wait_time = 30 * (attempt + 1)
                print(f"  > [AI] âš ï¸ è§¸ç™¼é™é€Ÿ (429)ï¼Œç­‰å¾… {wait_time} ç§’å¾Œé‡è©¦ ({attempt+1}/{max_retries})...")
                time.sleep(wait_time)
                continue
            else:
                print(f"  > [AI] API Error: {res.text}")
                return None
                
        except Exception as e:
            print(f"  > [AI] Request Error: {e}")
            time.sleep(10)
            continue
            
    print("  > [AI] âŒ é‡è©¦å¤šæ¬¡å¤±æ•—ï¼Œæ”¾æ£„æ­¤æ¢ç›®ã€‚")
    return None

# ==================== Supabase æ•´åˆ ====================
def init_supabase_client() -> Optional['Client']:
    """åˆå§‹åŒ– Supabase å®¢æˆ¶ç«¯ (éœ€è¦ç’°å¢ƒè®Šæ•¸ SUPABASE_URL & SUPABASE_KEY)"""
    if not SUPABASE_AVAILABLE:
        return None
    
    url = os.environ.get('SUPABASE_URL', '')
    key = os.environ.get('SUPABASE_KEY', '')
    
    if not url or not key:
        print("  > [Supabase] æœªè¨­å®š SUPABASE_URL æˆ– SUPABASE_KEY ç’°å¢ƒè®Šæ•¸ï¼Œè·³éé›²ç«¯åŒæ­¥ã€‚")
        return None
    
    try:
        client = create_client(url, key)
        print(f"  > [Supabase] âœ… æˆåŠŸé€£æ¥è‡³ {url[:30]}...")
        return client
    except Exception as e:
        print(f"  > [Supabase] âŒ é€£æ¥å¤±æ•—: {e}")
        return None

def resolve_google_news_url(url: str) -> str:
    """å˜—è©¦è§£æ Google News è½‰å€é€£çµç‚ºåŸå§‹ URL (æ¨¡æ“¬ç€è¦½å™¨ + é é¢å…§å®¹è§£æ)"""
    if "news.google.com" not in url:
        return url
    
    try:
        session = requests.Session()
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Referer": "https://news.google.com/",
        }
        
        # 1. å˜—è©¦ HTTP è«‹æ±‚
        resp = session.get(url, headers=headers, allow_redirects=True, timeout=10)
        
        # 2. æª¢æŸ¥æ˜¯å¦å·²ç¶“è·³è½‰é›¢é–‹ google
        if "news.google.com" not in resp.url:
            return resp.url
        
        # 3. å¦‚æœé‚„åœ¨ googleï¼Œå˜—è©¦å¾å…§å®¹ä¸­å°‹æ‰¾ç›®æ¨™é€£çµ
        # å¾ˆå¤šæ™‚å€™ google æœƒå›å‚³ä¸€å€‹é é¢ï¼Œä¸­é–“æœ‰ä¸€å€‹ <a href="..."> æˆ– <noscript>
        content = resp.text
        
        # ç­–ç•¥ A: æ‰¾ <a href="..." jsname="..."> (Google å¸¸è¦‹çš„è·³è½‰æŒ‰éˆ•)
        # å°‹æ‰¾æ‰€æœ‰é€£çµï¼Œä¸¦æ’é™¤ google è‡ªèº«çš„é€£çµ
        links = re.findall(r'href="([^"]+)"', content)
        candidates = [l for l in links if l.startswith('http') and "google.com" not in l and "google.cn" not in l]
        
        if candidates:
            # print(f"    [Page Parse] Found: {candidates[0][:50]}...")
            return candidates[0]
            
        # ç­–ç•¥ B: æ‰¾ window.location.replace("...")
        js_redirect = re.search(r'window\.location\.replace\("([^"]+)"\)', content)
        if js_redirect:
            return js_redirect.group(1)

    except Exception:
        pass

    # å¤±æ•—å›å‚³åŸç¶²å€
    return url

def clean_text(text: str) -> str:
    """æ¸…æ´—æ–‡å­—ï¼Œç§»é™¤å¯èƒ½å°è‡´ JSON éŒ¯èª¤çš„ç„¡æ•ˆå­—å…ƒ"""
    if not text: return ""
    # ç§»é™¤ null bytes å’Œæ§åˆ¶å­—å…ƒï¼Œä¸¦ç¢ºä¿æ˜¯ valid utf-8
    return text.encode('utf-8', 'ignore').decode('utf-8').replace('\x00', '')

def sync_to_supabase(supabase_client: 'Client', news_list: List[Dict]) -> int:
    """
    å°‡é«˜åƒ¹å€¼æ–°èåŒæ­¥åˆ° Supabase ai_news è¡¨ã€‚
    åŒ…å«ï¼šCheck-and-Insertã€é€£çµè§£æå¼·åŒ–ã€è³‡æ–™æ¸…æ´—
    """
    if not supabase_client or not news_list:
        return 0
    
    synced = 0
    total_items = len(news_list)
    print(f"  > [Supabase] æº–å‚™åŒæ­¥ {total_items} æ¢æ–°è (å«å…§å®¹æ·±åº¦è§£æ)...")
    
    ALLOWED_CATEGORIES = ["åŠå°é«”", "AI/ç§‘æŠ€", "é‡‘è", "å‚³ç”¢/èˆªé‹", "å®è§€/æ”¿ç­–", "å…¶å®ƒ"]
    
    for i, news in enumerate(news_list):
        try:
            # 0. è§£æçœŸå¯¦é€£çµ
            final_url = resolve_google_news_url(news['url'])
            
            # 0.5 å¼·åˆ¶æ¸…æ´—åˆ†é¡
            raw_cat = clean_text(news['category'])
            final_cat = raw_cat if raw_cat in ALLOWED_CATEGORIES else "å…¶å®ƒ"

            # 1. å…ˆæŸ¥è©¢æ˜¯å¦å­˜åœ¨
            existing = supabase_client.table('ai_news')\
                .select('id')\
                .eq('title', news['title'])\
                .eq('source', news['source'])\
                .execute()
            
            # æº–å‚™å¯«å…¥çš„è³‡æ–™ (å…¨éƒ¨æ¸…æ´—ä¸€é)
            data = {
                'title': clean_text(news['title']),
                'score': news['score'],
                'category': final_cat,
                'insight': clean_text(news['insight']),
                'url': clean_text(final_url),
                'source': clean_text(news['source']),
                'created_at': news['created_at']
            }

            if existing.data and len(existing.data) > 0:
                # 2. Update
                record_id = existing.data[0]['id']
                supabase_client.table('ai_news').update(data).eq('id', record_id).execute()
            else:
                # 3. Insert
                supabase_client.table('ai_news').insert(data).execute()
                
            synced += 1
        except Exception as e:
            # ç°¡åŒ–éŒ¯èª¤è¨Šæ¯ï¼Œé¿å…å°å‡ºå¤ªå¤š
            err_msg = str(e).replace('\n', ' ')
            print(f"  > [Supabase] åŒæ­¥å¤±æ•—: {news['title'][:10]}... Err: {err_msg[:100]}")
    
    return synced
# ========================================================

def prepare_data_sources() -> List[str]:
    """
    æº–å‚™æ•¸æ“šæºï¼š
    1. å˜—è©¦ä¸‹è¼‰ä»Šæ—¥çš„ 'news' å’Œ 'rss' è³‡æ–™åº«ã€‚
    2. å¦‚æœä»Šæ—¥çš†ç„¡ï¼Œå‰‡å°‹æ‰¾æœ¬åœ°æœ€æ–°çš„ 'news' è³‡æ–™åº«æ—¥æœŸä½œç‚º fallbackã€‚
    3. è¿”å›æœ‰æ•ˆçš„è³‡æ–™åº«è·¯å¾‘åˆ—è¡¨ã€‚
    """
    today_str = datetime.now().strftime('%Y-%m-%d')
    valid_dbs = []

    print(f"  > [Data Source] æª¢æŸ¥æ—¥æœŸ: {today_str}")

    # å˜—è©¦ä¸‹è¼‰ä»Šæ—¥æ•¸æ“š
    for db_type in ['news', 'rss']:
        path = download_from_r2(today_str, db_type)
        if path and os.path.exists(path):
            valid_dbs.append(path)
    
    # å¦‚æœä»Šæ—¥å®Œå…¨æ²’æ•¸æ“šï¼ˆå¯èƒ½æ˜¯æ—©ä¸Šå‰›é–‹å§‹æˆ–è€…æ˜¯æ¸¬è©¦ç’°å¢ƒï¼‰ï¼Œå›é€€æ‰¾æœ¬åœ°æœ€æ–°çš„
    if not valid_dbs:
        print("  > [Data Source] ä»Šæ—¥å°šç„¡æ•¸æ“šï¼Œæœå°‹æœ¬åœ°æ­·å²æª”æ¡ˆ...")
        files = [f for f in os.listdir('.') if f.startswith('trendradar_news_') and f.endswith('.db')]
        if files:
            latest_db = sorted(files)[-1]
            date_part = latest_db.replace('trendradar_news_', '').replace('.db', '')
            print(f"  > [Data Source] æ‰¾åˆ°æœ€æ–°æ­·å²æ—¥æœŸ: {date_part}")
            
            # ä½¿ç”¨è©²æ—¥æœŸé‡æ–°æª¢æŸ¥ news å’Œ rss
            valid_dbs.append(latest_db)
            rss_path = f'trendradar_rss_{date_part}.db'
            if os.path.exists(rss_path):
                valid_dbs.append(rss_path)
    
    return valid_dbs

def analyze_news_item(title: str) -> Dict:
    """ä½¿ç”¨ LLM åˆ†ææ–°èæ¨™é¡Œçš„æŠ•è³‡åƒ¹å€¼"""
    messages = [
        {
            "role": "system", 
            "content": "ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„è­‰åˆ¸åˆ†æå¸«ã€‚è«‹åˆ†ææ–°èæ¨™é¡Œï¼Œä¸¦çµ¦å‡ºæŠ•è³‡åƒ¹å€¼è©•åˆ†èˆ‡æ¿å¡Šæ­¸é¡ã€‚åš´æ ¼ä»¥ JSON æ ¼å¼å›è¦†ã€‚"
        },
        {
            "role": "user", 
            "content": f"""
æ–°èæ¨™é¡Œï¼š"{title}"

è«‹ä»¥åš´æ ¼çš„ã€ŒæŠ•è³‡äººè¦–è§’ã€é€²è¡Œåˆ†æï¼Œå›è¦† JSONï¼š
{{
  "score": 1-10 (æ•´æ•¸),
  "category": "åŠå°é«”" | "AI/ç§‘æŠ€" | "é‡‘è" | "å‚³ç”¢/èˆªé‹" | "ç”ŸæŠ€" | "å®è§€/æ”¿ç­–" | "å…¶å®ƒ",
  "reason": "ç°¡çŸ­ç†ç”±(20å­—å…§)"
}}

# è©•åˆ†æ¨™æº– (Score):
- **10åˆ† (Market Mover)**: é‡å¤§çªç™¼(å¦‚æˆ°çˆ­/é™æ¯)ã€å°ç©é›»/è¼é”è²¡å ±æš´é›·æˆ–é©šå–œã€åœ‹å®¶ç´šæ”¿ç­–è®Šå‹•ã€‚
- **8-9åˆ† (High Impact)**: æ¬Šå€¼è‚¡ç‡Ÿæ”¶å‰µæ–°é«˜/ä½ã€å¤§å‹ä½µè³¼ã€ç”¢æ¥­é¾é ­æ¼²è·Œåœã€‚
- **6-7åˆ† (Moderate)**: ä¸€èˆ¬å€‹è‚¡è²¡å ±ã€æ³•èªªæœƒæ¶ˆæ¯ã€å¤–è³‡å‡é™è©•ã€ç”¢æ¥­è¶¨å‹¢æ–°èã€‚
- **4-5åˆ† (Low)**: å€‹è‚¡ç›¤ä¸­æ³¢å‹•ã€ä¾‹è¡Œæ€§ç‡Ÿæ”¶å…¬å‘Š(ç„¡é©šå–œ)ã€è‚¡æ±æœƒæµæ°´å¸³ã€‚
- **1-3åˆ† (Noise)**: å»£å‘Šã€èŠ±é‚Šæ–°èã€èˆ‡ç¶“æ¿Ÿç„¡é—œçš„æ”¿æ²»å£æ°´ã€è¾²å ´æ¨™é¡Œã€‚

# æ³¨æ„äº‹é …:
- è‹¥æ¨™é¡ŒåŒ…å«ã€Œç›¤ä¸­é€Ÿå ±ã€ã€ã€Œè‚¡åƒ¹æ‹‰è‡³æ¼²åœã€ç­‰ï¼Œè¦–ç‚ºå³æ™‚è¡Œæƒ…ï¼Œçµ¦äºˆ 4-5 åˆ†ï¼ˆé™¤éæ˜¯æ¬Šå€¼è‚¡å¦‚å°ç©é›»/é´»æµ·å‰‡æ›´é«˜ï¼‰ã€‚
- åš´æ ¼éæ¿¾éè²¡ç¶“é¡æ–°è (å¦‚å¨›æ¨‚ã€é«”è‚²)ï¼Œç›´æ¥çµ¦ 1 åˆ†ã€‚
"""
        }
    ]
    try:
        response = call_openrouter(DEFAULT_MODEL, messages)
        if not response:
            return None
        
        # æ¸…ç†å¯èƒ½çš„ markdown æ¨™ç±¤
        clean_json = response.replace("```json", "").replace("```", "").strip()
        data = json.loads(clean_json)
        return data
    except Exception as e:
        print(f"  > [AI Filter] åˆ†æå¤±æ•—: {e}")
        return None

def init_investment_db():
    """åˆå§‹åŒ–æœ¬åœ°æŠ•è³‡æ–°èè³‡æ–™åº«"""
    conn = sqlite3.connect(INVESTMENT_DB)
    cursor = conn.cursor()
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS processed_news (
            original_id INTEGER,
            platform_name TEXT,
            title TEXT,
            url TEXT,
            score INTEGER,
            category TEXT,
            analysis TEXT,
            crawl_time TEXT,
            processed_at TEXT,
            PRIMARY KEY (original_id, platform_name)
        )
    ''')
    conn.commit()
    conn.close()

def process_latest_news():
    """ä¸»è™•ç†æµç¨‹"""
    db_paths = prepare_data_sources()
    if not db_paths:
        print("  > [AI Filter] âŒ æ‰¾ä¸åˆ°ä»»ä½•è³‡æ–™åº« (News æˆ– RSS)ã€‚")
        return

    print(f"  > [AI Filter] å°‡è™•ç†ä»¥ä¸‹è³‡æ–™åº«: {db_paths}")
    init_investment_db()
    
    # å½™æ•´æ‰€æœ‰ä¾†æºçš„æ–°è
    all_rows = []
    
    for db_path in db_paths:
        try:
            print(f"  > [AI Filter] è®€å– {db_path} ...")
            conn_raw = sqlite3.connect(db_path)
            conn_raw.row_factory = sqlite3.Row
            cursor_raw = conn_raw.cursor()
            
            # åˆ¤æ–·æ˜¯å¦ç‚º RSS ä¾†æº (é€éæª”å)
            is_rss = 'rss' in db_path
            source_label = 'RSS' if is_rss else 'News'
            
            # å®šç¾©ä¸åŒè³‡æ–™åº«é¡å‹çš„è¡¨å
            table_name = "rss_items" if is_rss else "news_items"
            platform_table = "rss_feeds" if is_rss else "platforms"
            platform_id_col = "feed_id" if is_rss else "platform_id"
            
            # å–å¾—æ–°è
            cursor_raw.execute(f"""
                SELECT n.id, n.title, n.url, n.first_crawl_time, p.name as platform_name 
                FROM {table_name} n
                LEFT JOIN {platform_table} p ON n.{platform_id_col} = p.id
                ORDER BY n.id DESC
                LIMIT 200
            """)
            rows = cursor_raw.fetchall()
            
            # å°‡ row è½‰æ›ç‚º dict ä¸¦åŠ ä¸Šä¾†æºæ¨™è¨˜ï¼Œæ–¹ä¾¿å¾ŒçºŒè™•ç†
            for r in rows:
                item = dict(r)
                item['source_db'] = source_label
                # è‹¥ RSS çš„ platform_name ç‚ºç©ºï¼Œä½¿ç”¨é è¨­å€¼
                if not item['platform_name']:
                    item['platform_name'] = f"{source_label}_Feed"
                all_rows.append(item)
                
            conn_raw.close()
        except Exception as e:
            print(f"  > [AI Filter] âš ï¸ è®€å– {db_path} å¤±æ•—: {e}")

    print(f"  > [AI Filter] å…±å–å¾— {len(all_rows)} æ¢åŸå§‹æ–°èï¼Œæº–å‚™é€²è¡Œåˆ†æ...")

    # é€£æ¥åˆ°è¼¸å‡ºåº«
    conn_inv = sqlite3.connect(INVESTMENT_DB)
    cursor_inv = conn_inv.cursor()

    count_processed = 0
    count_high_value = 0

    for row in all_rows:
        # æª¢æŸ¥æ˜¯å¦å·²è™•ç†é
        cursor_inv.execute("SELECT 1 FROM processed_news WHERE original_id=? AND platform_name=?", 
                          (row['id'], row['platform_name']))
        if cursor_inv.fetchone():
            continue

        print(f"  > [AI Filter] æ­£åœ¨åˆ†æ: {row['title'][:30]}...")
        analysis = analyze_news_item(row['title'])
        
        # === é—œéµä¿®æ”¹ï¼šå¼·åˆ¶ä¼‘æ¯ ===
        # OpenRouter å…è²»ç‰ˆé™åˆ¶ç´„ 20 req/minï¼Œæ‰€ä»¥æ¯æ¬¡ä¼‘æ¯ 10 ç§’ + åŸ·è¡Œæ™‚é–“ï¼Œå‰›å¥½å®‰å…¨
        time.sleep(10) 
        # ========================

        if not analysis:
            continue
            
        score = analysis.get('score', 0)
        raw_cat = analysis.get('category', 'å…¶å®ƒ')
        reason = analysis.get('reason', '')
        
        # Lovable Cloud Category Constraint Fix
        # è³‡æ–™åº«æœ‰ CHECK ç´„æŸï¼Œå¿…é ˆå°æ‡‰å…è¨±çš„æ¸…å–®
        ALLOWED_CATEGORIES = ["åŠå°é«”", "AI/ç§‘æŠ€", "é‡‘è", "å‚³ç”¢/èˆªé‹", "å®è§€/æ”¿ç­–", "å…¶å®ƒ"]
        category = raw_cat if raw_cat in ALLOWED_CATEGORIES else "å…¶å®ƒ"

        # å­˜å…¥è³‡æ–™åº«
        cursor_inv.execute("""
            INSERT INTO processed_news 
            (original_id, platform_name, title, url, score, category, analysis, crawl_time, processed_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            row['id'], 
            row['platform_name'], 
            row['title'], 
            row['url'], 
            score, 
            category, 
            reason, 
            row['first_crawl_time'],
            datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        ))
        
        count_processed += 1
        if score >= MIN_SCORE_THRESHOLD:
            count_high_value += 1
            print(f"    ğŸŒŸ é«˜åƒ¹å€¼ç™¼ç¾! [{category}] åˆ†æ•¸: {score} - {reason}")

    conn_inv.commit()
    conn_inv.close()
    
    print(f"\nâœ… æœ¬åœ°è™•ç†å®Œæˆï¼")
    print(f"   â€¢ æ–°å¢è™•ç†: {count_processed} æ¢")
    print(f"   â€¢ å…¶ä¸­é«˜åƒ¹å€¼ (>={MIN_SCORE_THRESHOLD}): {count_high_value} æ¢")
    
    # ===== åŒæ­¥è‡³ Supabase (å¯é¸) =====
    supabase = init_supabase_client()
    if supabase:
        # å¾æœ¬åœ°è³‡æ–™åº«è®€å–ä»Šæ—¥é«˜åˆ†æ–°è
        conn_sync = sqlite3.connect(INVESTMENT_DB)
        conn_sync.row_factory = sqlite3.Row
        cursor_sync = conn_sync.cursor()
        today_str = datetime.now().strftime('%Y-%m-%d')
        cursor_sync.execute(f"""
            SELECT title, score, category, analysis as insight, url, platform_name as source, processed_at as created_at
            FROM processed_news
            WHERE score >= {MIN_SCORE_THRESHOLD} AND processed_at LIKE '{today_str}%'
        """)
        high_value_news = [dict(row) for row in cursor_sync.fetchall()]
        conn_sync.close()
        
        if high_value_news:
            synced_count = sync_to_supabase(supabase, high_value_news)
            print(f"\nâ˜ï¸ Supabase åŒæ­¥å®Œæˆï¼å…±æ¨é€ {synced_count} æ¢é«˜åƒ¹å€¼æ–°èã€‚")
        else:
            print("\nâ˜ï¸ ä»Šæ—¥ç„¡æ–°å¢é«˜åƒ¹å€¼æ–°èéœ€åŒæ­¥è‡³ Supabaseã€‚")

if __name__ == "__main__":
    initialize_services()
    process_latest_news()
