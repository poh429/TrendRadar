# --- step39_news_filter.py (Truly Standalone Version v3.0) ---
"""
Vestra Data Utility Engine - AI æ–°èéæ¿¾èˆ‡è©•åˆ†æ¨¡çµ„
Standalone ç‰ˆæœ¬ï¼šå…§å»º AI å‘¼å« + R2 ä¸‹è¼‰é‚è¼¯ï¼Œä¸ä¾è³´ä»»ä½•å¤–éƒ¨å°ˆæ¡ˆæª”ã€‚
"""

import sqlite3
import os
import json
import requests
import re
import base64
import time
from datetime import datetime
from typing import List, Dict, Tuple, Optional

# R2 ä¸‹è¼‰åŠŸèƒ½éœ€è¦ boto3
try:
    import boto3
    from botocore.client import Config
    BOTO3_AVAILABLE = True
except ImportError:
    BOTO3_AVAILABLE = False
    print("  > [System] æœªå®‰è£ boto3ï¼Œç„¡æ³•å¾ R2 ä¸‹è¼‰æ•¸æ“šã€‚")

# Supabase æ•´åˆ
try:
    from supabase import create_client, Client
    SUPABASE_AVAILABLE = True
except ImportError:
    SUPABASE_AVAILABLE = False

# é…ç½®å€
INVESTMENT_DB = "investment_news.db"
# å…è²»æ¨¡å‹å„ªå…ˆé †åºåˆ—è¡¨ (ç•¶é‡åˆ° 429 æ™‚è‡ªå‹•åˆ‡æ›)
FREE_MODELS = [
    "google/gemini-2.0-flash-exp:free",       # é¦–é¸ï¼šGoogle æœ€æ–°ã€é€Ÿåº¦å¿«
    "xiaomi/mimo-v2-flash:free",              # å°ç±³ MoE 309Bï¼Œæ¨ç†èƒ½åŠ›é ‚ç´š
    "tngtech/deepseek-r1t2-chimera:free",     # DeepSeek R1T2 Chimera (671B MoE)
    "nex-agi/nex-n1-deepseek-v3.1:free",      # DeepSeek V3.1 æ——è‰¦ç³»åˆ—
    "google/gemma-3-27b-it:free",             # Google Gemma 27B
    "meta-llama/llama-3.3-70b-instruct:free", # Meta Llama 3.3
    "meta-llama/llama-3.1-405b-instruct:free", # Meta æœ€å¤§æ¨¡å‹
    "nvidia/nemotron-3-nano-30b-a3b:free",    # NVIDIA 30B MoE (é«˜æ•ˆç‡)
    "mistralai/devstral-2-2512:free",         # Mistral 123B (Coding å°ˆå®¶)
    "kwaipilot/kat-coder-pro:free",           # KAT-Coder-Pro (Agentic Coding)
]
DEFAULT_MODEL = FREE_MODELS[0]
MIN_SCORE_THRESHOLD = 5

# ==================== 1. å…§å»º R2 ä¸‹è¼‰åŠŸèƒ½ ====================
def download_from_r2(date_str, db_type='news'):
    """å¾ R2 ä¸‹è¼‰æŒ‡å®šæ—¥æœŸçš„è³‡æ–™åº« (å…§å»ºç‰ˆ)"""
    if not BOTO3_AVAILABLE:
        return None
        
    # å¾ç’°å¢ƒè®Šæ•¸è®€å–é…ç½®
    r2_endpoint = os.environ.get("S3_ENDPOINT_URL")
    r2_key_id = os.environ.get("S3_ACCESS_KEY_ID")
    r2_secret = os.environ.get("S3_SECRET_ACCESS_KEY")
    bucket_name = os.environ.get("S3_BUCKET_NAME", "trendradar-news")
    
    if not (r2_endpoint and r2_key_id and r2_secret):
        print("  > [R2] ç¼ºå°‘ R2 ç’°å¢ƒè®Šæ•¸ (S3_ENDPOINT_URL ç­‰)")
        return None

    try:
        s3 = boto3.client('s3',
            endpoint_url=r2_endpoint,
            aws_access_key_id=r2_key_id,
            aws_secret_access_key=r2_secret,
            config=Config(signature_version='s3v4'),
            region_name='auto'
        )
        
        file_name = f'trendradar_{db_type}_{date_str}.db'
        object_key = f'{db_type}/{date_str}.db'
        local_path = file_name
        
        print(f"  > [R2] æ­£åœ¨ä¸‹è¼‰: {object_key} -> {local_path} ...")
        s3.download_file(bucket_name, object_key, local_path)
        print(f"  > [R2] âœ… ä¸‹è¼‰æˆåŠŸ: {local_path}")
        return local_path
        
    except Exception as e:
        print(f"  > [R2] âš ï¸ ä¸‹è¼‰å¤±æ•— ({object_key}): {e}")
        return None

# ==================== 2. å…§å»º AI æ ¸å¿ƒ (Rate Limited) ====================
def initialize_services():
    """ç‚ºäº†ç›¸å®¹æ€§ä¿ç•™ï¼Œå¯¦éš›ä¸Šä¸éœ€è¦åšå¤ªå¤šäº‹"""
    pass

def call_openrouter(model, messages, temperature=0.3):
    """
    å…§å»º OpenRouter Caller (å«å¤šæ¨¡å‹ Fallback æ©Ÿåˆ¶)
    ç•¶é‡åˆ° 429 é™é€Ÿæ™‚ï¼Œè‡ªå‹•åˆ‡æ›åˆ°ä¸‹ä¸€å€‹å…è²»æ¨¡å‹å˜—è©¦
    """
    key = os.environ.get("OPENROUTER_API_KEY")
    if not key:
        print("  > [AI] âŒ ç¼ºå°‘ OPENROUTER_API_KEY")
        return None
    
    # å»ºç«‹è¦å˜—è©¦çš„æ¨¡å‹åˆ—è¡¨ (å¾å‚³å…¥çš„ model é–‹å§‹)
    models_to_try = [model]
    for m in FREE_MODELS:
        if m not in models_to_try:
            models_to_try.append(m)
    
    # å˜—è©¦æ¯å€‹æ¨¡å‹
    for current_model in models_to_try:
        max_retries = 2  # æ¯å€‹æ¨¡å‹æœ€å¤šé‡è©¦ 2 æ¬¡
        
        for attempt in range(max_retries):
            try:
                res = requests.post(
                    "https://openrouter.ai/api/v1/chat/completions",
                    headers={
                        "Authorization": f"Bearer {key}",
                        "HTTP-Referer": "http://localhost:8501",
                        "X-Title": "Vestra AI Filter"
                    },
                    json={"model": current_model, "messages": messages, "temperature": temperature},
                    timeout=60
                )
                
                if res.status_code == 200: 
                    return res.json()['choices'][0]['message']['content']
                elif res.status_code == 429:
                    if attempt < max_retries - 1:
                        wait_time = 5 * (attempt + 1)
                        print(f"  > [AI] âš ï¸ {current_model.split('/')[1][:15]} é™é€Ÿï¼Œç­‰å¾… {wait_time}s...")
                        time.sleep(wait_time)
                    else:
                        # æ­¤æ¨¡å‹é‡è©¦å®Œç•¢ï¼Œå˜—è©¦ä¸‹ä¸€å€‹æ¨¡å‹
                        next_idx = models_to_try.index(current_model) + 1
                        if next_idx < len(models_to_try):
                            print(f"  > [AI] ğŸ”„ åˆ‡æ›æ¨¡å‹: {models_to_try[next_idx].split('/')[1][:20]}")
                        break
                else:
                    print(f"  > [AI] API Error ({res.status_code}): {res.text[:100]}")
                    return None
                    
            except Exception as e:
                print(f"  > [AI] Request Error: {e}")
                time.sleep(5)
                continue
    
    print("  > [AI] âŒ æ‰€æœ‰æ¨¡å‹çš†å¤±æ•—ï¼Œæ”¾æ£„æ­¤æ¢ç›®ã€‚")
    return None

# ==================== Supabase æ•´åˆ ====================
def init_supabase_client() -> Optional['Client']:
    """åˆå§‹åŒ– Supabase å®¢æˆ¶ç«¯ (éœ€è¦ç’°å¢ƒè®Šæ•¸ SUPABASE_URL & SUPABASE_KEY)"""
    if not SUPABASE_AVAILABLE:
        return None
    
    url = os.environ.get('SUPABASE_URL', '')
    key = os.environ.get('SUPABASE_KEY', '')
    
    if not url or not key:
        print("  > [Supabase] æœªè¨­å®š SUPABASE_URL æˆ– SUPABASE_KEY ç’°å¢ƒè®Šæ•¸ï¼Œè·³éé›²ç«¯åŒæ­¥ã€‚")
        return None
    
    try:
        client = create_client(url, key)
        print(f"  > [Supabase] âœ… æˆåŠŸé€£æ¥è‡³ {url[:30]}...")
        return client
    except Exception as e:
        print(f"  > [Supabase] âŒ é€£æ¥å¤±æ•—: {e}")
        return None

def resolve_google_news_url(url: str) -> str:
    """
    Google News URL è™•ç†
    
    ç¶“æ¸¬è©¦ç¢ºèªï¼šGoogle News RSS URL (å¦‚ /articles/CBMi...) 
    åœ¨ç€è¦½å™¨ä¸­é»æ“Šæ™‚æœƒè‡ªå‹•é‡å°å‘åˆ°åŸå§‹æ–°èç¶²ç«™ã€‚
    
    å› æ­¤ä¸éœ€è¦è¤‡é›œçš„è§£ç¢¼é‚è¼¯ï¼Œç›´æ¥è¿”å›åŸ URL å³å¯ã€‚
    ä½¿ç”¨è€…åœ¨ Lovable å‰ç«¯é»æ“Šé€£çµæ™‚ï¼Œç€è¦½å™¨æœƒè‡ªå‹•è™•ç†é‡å°å‘ã€‚
    """
    return url

def clean_text(text: str) -> str:
    """æ¸…æ´—æ–‡å­—ï¼Œç§»é™¤å¯èƒ½å°è‡´ JSON éŒ¯èª¤çš„ç„¡æ•ˆå­—å…ƒ"""
    if not text: return ""
    # ç§»é™¤ null bytes å’Œæ§åˆ¶å­—å…ƒï¼Œä¸¦ç¢ºä¿æ˜¯ valid utf-8
    return text.encode('utf-8', 'ignore').decode('utf-8').replace('\x00', '')

def sync_to_supabase(supabase_client: 'Client', news_list: List[Dict]) -> int:
    """
    å°‡é«˜åƒ¹å€¼æ–°èåŒæ­¥åˆ° Supabase ai_news è¡¨ã€‚
    åŒ…å«ï¼šCheck-and-Insertã€é€£çµè§£æå¼·åŒ–ã€è³‡æ–™æ¸…æ´—
    """
    if not supabase_client or not news_list:
        return 0
    
    synced = 0
    total_items = len(news_list)
    print(f"  > [Supabase] æº–å‚™åŒæ­¥ {total_items} æ¢æ–°è (å«å…§å®¹æ·±åº¦è§£æ)...")
    
    ALLOWED_CATEGORIES = ["åŠå°é«”", "AI/ç§‘æŠ€", "é‡‘è", "å‚³ç”¢/èˆªé‹", "å®è§€/æ”¿ç­–", "å…¶å®ƒ"]
    
    for i, news in enumerate(news_list):
        try:
            # 0. è§£æçœŸå¯¦é€£çµ
            final_url = resolve_google_news_url(news['url'])
            
            # 0.5 å¼·åˆ¶æ¸…æ´—åˆ†é¡
            raw_cat = clean_text(news['category'])
            final_cat = raw_cat if raw_cat in ALLOWED_CATEGORIES else "å…¶å®ƒ"

            # 1. å…ˆæŸ¥è©¢æ˜¯å¦å­˜åœ¨
            existing = supabase_client.table('ai_news')\
                .select('id')\
                .eq('title', news['title'])\
                .eq('source', news['source'])\
                .execute()
            
            # æº–å‚™å¯«å…¥çš„è³‡æ–™ (å…¨éƒ¨æ¸…æ´—ä¸€é)
            data = {
                'title': clean_text(news['title']),
                'score': news['score'],
                'category': final_cat,
                'insight': clean_text(news['insight']),
                'url': clean_text(final_url),
                'source': clean_text(news['source']),
                'created_at': news['created_at']
            }

            if existing.data and len(existing.data) > 0:
                # 2. Update
                record_id = existing.data[0]['id']
                supabase_client.table('ai_news').update(data).eq('id', record_id).execute()
            else:
                # 3. Insert
                supabase_client.table('ai_news').insert(data).execute()
                
            synced += 1
        except Exception as e:
            # ç°¡åŒ–éŒ¯èª¤è¨Šæ¯ï¼Œé¿å…å°å‡ºå¤ªå¤š
            err_msg = str(e).replace('\n', ' ')
            print(f"  > [Supabase] åŒæ­¥å¤±æ•—: {news['title'][:10]}... Err: {err_msg[:100]}")
    
    return synced
# ========================================================

def prepare_data_sources() -> List[str]:
    """
    æº–å‚™æ•¸æ“šæºï¼š
    1. å˜—è©¦ä¸‹è¼‰ä»Šæ—¥çš„ 'news' å’Œ 'rss' è³‡æ–™åº«ã€‚
    2. å¦‚æœä»Šæ—¥çš†ç„¡ï¼Œå‰‡å°‹æ‰¾æœ¬åœ°æœ€æ–°çš„ 'news' è³‡æ–™åº«æ—¥æœŸä½œç‚º fallbackã€‚
    3. è¿”å›æœ‰æ•ˆçš„è³‡æ–™åº«è·¯å¾‘åˆ—è¡¨ã€‚
    """
    today_str = datetime.now().strftime('%Y-%m-%d')
    valid_dbs = []

    print(f"  > [Data Source] æª¢æŸ¥æ—¥æœŸ: {today_str}")

    # å˜—è©¦ä¸‹è¼‰ä»Šæ—¥æ•¸æ“š
    for db_type in ['news', 'rss']:
        path = download_from_r2(today_str, db_type)
        if path and os.path.exists(path):
            valid_dbs.append(path)
    
    # å¦‚æœä»Šæ—¥å®Œå…¨æ²’æ•¸æ“šï¼ˆå¯èƒ½æ˜¯æ—©ä¸Šå‰›é–‹å§‹æˆ–è€…æ˜¯æ¸¬è©¦ç’°å¢ƒï¼‰ï¼Œå›é€€æ‰¾æœ¬åœ°æœ€æ–°çš„
    if not valid_dbs:
        print("  > [Data Source] ä»Šæ—¥å°šç„¡æ•¸æ“šï¼Œæœå°‹æœ¬åœ°æ­·å²æª”æ¡ˆ...")
        files = [f for f in os.listdir('.') if f.startswith('trendradar_news_') and f.endswith('.db')]
        if files:
            latest_db = sorted(files)[-1]
            date_part = latest_db.replace('trendradar_news_', '').replace('.db', '')
            print(f"  > [Data Source] æ‰¾åˆ°æœ€æ–°æ­·å²æ—¥æœŸ: {date_part}")
            
            # ä½¿ç”¨è©²æ—¥æœŸé‡æ–°æª¢æŸ¥ news å’Œ rss
            valid_dbs.append(latest_db)
            rss_path = f'trendradar_rss_{date_part}.db'
            if os.path.exists(rss_path):
                valid_dbs.append(rss_path)
    
    return valid_dbs

def analyze_news_item(title: str) -> Dict:
    """ä½¿ç”¨ LLM åˆ†ææ–°èæ¨™é¡Œçš„æŠ•è³‡åƒ¹å€¼"""
    messages = [
        {
            "role": "system", 
            "content": "ä½ æ˜¯ä¸€ä½å°ˆç²¾æ–¼ã€Œå°è‚¡ã€èˆ‡ã€Œç¾è‚¡ã€çš„é¦–å¸­æŠ•è³‡åˆ†æå¸«ã€‚ä½ çš„ä»»å‹™æ˜¯å¾é›œäº‚çš„æ–°èä¸­ï¼Œç¯©é¸å‡ºå°å°ç£è‚¡å¸‚(TWSE)æˆ–ç¾åœ‹è‚¡å¸‚(NASDAQ/NYSE/S&P)æœ‰å¯¦è³ªå½±éŸ¿çš„æƒ…å ±ã€‚åš´æ ¼ä»¥ JSON æ ¼å¼å›è¦†ã€‚"
        },
        {
            "role": "user", 
            "content": f"""
æ–°èæ¨™é¡Œï¼š"{title}"

è«‹ä»¥åš´æ ¼çš„ã€Œå°/ç¾è‚¡æŠ•è³‡äººè¦–è§’ã€é€²è¡Œåˆ†æï¼Œå›è¦† JSONï¼š
{{
  "score": 1-10 (æ•´æ•¸),
  "category": "åŠå°é«”" | "AI/ç§‘æŠ€" | "é‡‘è" | "å‚³ç”¢/èˆªé‹" | "ç”ŸæŠ€" | "å®è§€/æ”¿ç­–" | "å…¶å®ƒ",
  "reason": "ç°¡çŸ­ç†ç”±(20å­—å…§)"
}}

# è©•åˆ†æ¨™æº– (Score):
- **10åˆ† (Market Mover)**: é‡å¤§çªç™¼(å¦‚æˆ°çˆ­/Fedé™æ¯)ã€å°ç©é›»/è¼é”/è˜‹æœ/AMDè²¡å ±æš´é›·æˆ–é©šå–œã€åœ‹å®¶ç´šæ”¿ç­–ç›´æ¥å½±éŸ¿ä¾›æ‡‰éˆã€‚
- **8-9åˆ† (High Impact)**: å°/ç¾æ¬Šå€¼è‚¡(å¦‚é´»æµ·, è¯ç™¼ç§‘, Tesla, MSFT)ç‡Ÿæ”¶èˆ‡å‹•æ…‹ã€å¤§å‹ä½µè³¼ã€‚
- **6-7åˆ† (Moderate)**: åŠå°é«”/AIä¾›æ‡‰éˆæ¶ˆæ¯ã€ç¾è‚¡ç§‘æŠ€å·¨é ­å‹•æ…‹ã€åŒæ¥­ç«¶çˆ­ã€é—œéµåŸç‰©æ–™åƒ¹æ ¼ã€‚
- **4-5åˆ† (Low)**: ä¸€èˆ¬å€‹è‚¡æ³¢å‹•ã€éæ ¸å¿ƒç”¢æ¥­(å¦‚é™¸è‚¡ç™½é…’/å…§éœ€)ã€ä¾‹è¡Œæ€§å…¬å‘Šã€‚
- **1-3åˆ† (Noise)**: **ä¸­åœ‹å…§åœ°ç¤¾æœƒæ–°è(å¦‚ç¤¾æœƒæ¡ˆä»¶)**ã€**èˆ‡å°ç¾è‚¡ç„¡é—œçš„æ”¿æ²»å£æ°´**ã€å»£å‘Šã€ç´”å¨›æ¨‚ã€é«”è‚²ã€‚

# é—œéµéæ¿¾è¦å‰‡:
1. **ä¸»è¦é—œæ³¨**: å°ç£ä¼æ¥­ã€ç¾åœ‹ç§‘æŠ€å·¨é ­ã€å…¨çƒå®è§€ç¶“æ¿Ÿ(é€šè†¨/æ²¹åƒ¹/ç¾å‚µ)ã€‚
2. **è‡ªå‹•é™ç´š**: è‹¥æ–°èåƒ…æ¶‰åŠã€Œä¸­åœ‹Aè‚¡ç‰¹å®šæ¿å¡Š(å¦‚èŒ…å°ã€Aè‚¡å…§è³‡)ã€ä¸”ç„¡å…¨çƒå½±éŸ¿åŠ›ï¼Œè«‹çµ¦äºˆ 3 åˆ†ä»¥ä¸‹ã€‚
3. **å™ªéŸ³å‰”é™¤**: ç¤¾æœƒæ¡ˆä»¶(å¦‚æ®ºäºº/è»Šç¦)ã€éè²¡ç¶“é¡æ”¿æ²»æ–°è(ç„¡ç¶“æ¿Ÿåˆ¶è£å…§å®¹)ï¼Œä¸€å¾‹ 1 åˆ†ã€‚
"""
        }
    ]
    try:
        response = call_openrouter(DEFAULT_MODEL, messages)
        if not response:
            return None
        
        # æ¸…ç†å¯èƒ½çš„ markdown æ¨™ç±¤
        clean_json = response.replace("```json", "").replace("```", "").strip()
        data = json.loads(clean_json)
        
        # å¦‚æœ AI å›å‚³çš„æ˜¯ listï¼Œå–ç¬¬ä¸€å€‹å…ƒç´ 
        if isinstance(data, list) and len(data) > 0:
            data = data[0]
        
        # ç¢ºä¿æœ‰å¿…è¦çš„æ¬„ä½
        if isinstance(data, dict) and 'score' in data:
            return data
        
        return None
    except Exception as e:
        print(f"  > [AI Filter] åˆ†æå¤±æ•—: {e}")
        return None

def init_investment_db():
    """åˆå§‹åŒ–æœ¬åœ°æŠ•è³‡æ–°èè³‡æ–™åº«"""
    conn = sqlite3.connect(INVESTMENT_DB)
    cursor = conn.cursor()
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS processed_news (
            original_id INTEGER,
            platform_name TEXT,
            title TEXT,
            url TEXT,
            score INTEGER,
            category TEXT,
            analysis TEXT,
            crawl_time TEXT,
            processed_at TEXT,
            PRIMARY KEY (original_id, platform_name)
        )
    ''')
    conn.commit()
    conn.close()

def process_latest_news():
    """ä¸»è™•ç†æµç¨‹"""
    db_paths = prepare_data_sources()
    if not db_paths:
        print("  > [AI Filter] âŒ æ‰¾ä¸åˆ°ä»»ä½•è³‡æ–™åº« (News æˆ– RSS)ã€‚")
        return

    print(f"  > [AI Filter] å°‡è™•ç†ä»¥ä¸‹è³‡æ–™åº«: {db_paths}")
    init_investment_db()
    
    # å½™æ•´æ‰€æœ‰ä¾†æºçš„æ–°èï¼Œä¸¦åš´æ ¼é™åˆ¶ç¸½é‡
    all_rows = []
    GLOBAL_LIMIT = 60  # å…¨å±€ç¸½é™åˆ¶ (æ¯æ¬¡æœ€å¤šè™•ç† 60 æ¢)
    
    for db_path in db_paths:
        if len(all_rows) >= GLOBAL_LIMIT:
            break
            
        try:
            print(f"  > [AI Filter] è®€å– {db_path} ...")
            conn_raw = sqlite3.connect(db_path)
            conn_raw.row_factory = sqlite3.Row
            cursor_raw = conn_raw.cursor()
            
            # åˆ¤æ–·æ˜¯å¦ç‚º RSS ä¾†æº (é€éæª”å)
            is_rss = 'rss' in db_path
            source_label = 'RSS' if is_rss else 'News'
            
            # å®šç¾©ä¸åŒè³‡æ–™åº«é¡å‹çš„è¡¨å
            table_name = "rss_items" if is_rss else "news_items"
            platform_table = "rss_feeds" if is_rss else "platforms"
            platform_id_col = "feed_id" if is_rss else "platform_id"
            
            # è¨ˆç®—å‰©é¤˜é¡åº¦
            remaining_limit = GLOBAL_LIMIT - len(all_rows)
            
            # å–å¾—æ–°è
            cursor_raw.execute(f"""
                SELECT n.id, n.title, n.url, n.first_crawl_time, p.name as platform_name 
                FROM {table_name} n
                LEFT JOIN {platform_table} p ON n.{platform_id_col} = p.id
                ORDER BY n.id DESC
                LIMIT {remaining_limit}
            """)
            rows = cursor_raw.fetchall()
            
            # å°‡ row è½‰æ›ç‚º dict ä¸¦åŠ ä¸Šä¾†æºæ¨™è¨˜ï¼Œæ–¹ä¾¿å¾ŒçºŒè™•ç†
            for r in rows:
                item = dict(r)
                # çµ±ä¸€æ™‚é–“æ¬„ä½
                item['created_at'] = item['first_crawl_time'] 
                item['source'] = f"{source_label}-{item['platform_name']}"
                all_rows.append(item)
                
            conn_raw.close()
            
        except Exception as e:
            print(f"  > [Data Source] è®€å– {db_path} éŒ¯èª¤: {e}")

    print(f"  > [AI Filter] å…±å–å¾— {len(all_rows)} æ¢åŸå§‹æ–°è (Global Limit: {GLOBAL_LIMIT})ï¼Œæº–å‚™é€²è¡Œåˆ†æ...")

    # é€£æ¥åˆ°è¼¸å‡ºåº«
    conn_inv = sqlite3.connect(INVESTMENT_DB)
    cursor_inv = conn_inv.cursor()

    count_processed = 0
    count_high_value = 0

    for row in all_rows:
        # æª¢æŸ¥æ˜¯å¦å·²è™•ç†é
        cursor_inv.execute("SELECT 1 FROM processed_news WHERE original_id=? AND platform_name=?", 
                          (row['id'], row['platform_name']))
        if cursor_inv.fetchone():
            continue

        print(f"  > [AI Filter] æ­£åœ¨åˆ†æ: {row['title'][:30]}...")
        analysis = analyze_news_item(row['title'])
        
        # === è«‹æ±‚é–“éš” ===
        # é¿å…è§¸ç™¼ OpenRouter æ¯åˆ†é˜è«‹æ±‚é™åˆ¶
        time.sleep(10)

        if not analysis:
            continue
            
        score = analysis.get('score', 0)
        raw_cat = analysis.get('category', 'å…¶å®ƒ')
        reason = analysis.get('reason', '')
        
        # Lovable Cloud Category Constraint Fix
        # è³‡æ–™åº«æœ‰ CHECK ç´„æŸï¼Œå¿…é ˆå°æ‡‰å…è¨±çš„æ¸…å–®
        ALLOWED_CATEGORIES = ["åŠå°é«”", "AI/ç§‘æŠ€", "é‡‘è", "å‚³ç”¢/èˆªé‹", "å®è§€/æ”¿ç­–", "å…¶å®ƒ"]
        category = raw_cat if raw_cat in ALLOWED_CATEGORIES else "å…¶å®ƒ"

        # å­˜å…¥è³‡æ–™åº«
        cursor_inv.execute("""
            INSERT INTO processed_news 
            (original_id, platform_name, title, url, score, category, analysis, crawl_time, processed_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            row['id'], 
            row['platform_name'], 
            row['title'], 
            row['url'], 
            score, 
            category, 
            reason, 
            row['first_crawl_time'],
            datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        ))
        
        count_processed += 1
        if score >= MIN_SCORE_THRESHOLD:
            count_high_value += 1
            print(f"    ğŸŒŸ é«˜åƒ¹å€¼ç™¼ç¾! [{category}] åˆ†æ•¸: {score} - {reason}")

    conn_inv.commit()
    conn_inv.close()
    
    print(f"\nâœ… æœ¬åœ°è™•ç†å®Œæˆï¼")
    print(f"   â€¢ æ–°å¢è™•ç†: {count_processed} æ¢")
    print(f"   â€¢ å…¶ä¸­é«˜åƒ¹å€¼ (>={MIN_SCORE_THRESHOLD}): {count_high_value} æ¢")
    
    # ===== åŒæ­¥è‡³ Supabase (å¯é¸) =====
    supabase = init_supabase_client()
    if supabase:
        # å¾æœ¬åœ°è³‡æ–™åº«è®€å–ä»Šæ—¥é«˜åˆ†æ–°è
        conn_sync = sqlite3.connect(INVESTMENT_DB)
        conn_sync.row_factory = sqlite3.Row
        cursor_sync = conn_sync.cursor()
        today_str = datetime.now().strftime('%Y-%m-%d')
        cursor_sync.execute(f"""
            SELECT title, score, category, analysis as insight, url, platform_name as source, processed_at as created_at
            FROM processed_news
            WHERE score >= {MIN_SCORE_THRESHOLD} AND processed_at LIKE '{today_str}%'
        """)
        high_value_news = [dict(row) for row in cursor_sync.fetchall()]
        conn_sync.close()
        
        if high_value_news:
            synced_count = sync_to_supabase(supabase, high_value_news)
            print(f"\nâ˜ï¸ Supabase åŒæ­¥å®Œæˆï¼å…±æ¨é€ {synced_count} æ¢é«˜åƒ¹å€¼æ–°èã€‚")
            
            # ===== æ¸…ç†èˆŠè³‡æ–™ (ä¿ç•™ 7 å¤© + æœ€å¤š 100 æ¢) =====
            try:
                from datetime import timedelta
                cutoff_date = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')
                
                # 1. åˆªé™¤è¶…é 7 å¤©çš„èˆŠæ–°è
                supabase.table("ai_news").delete().lt("created_at", cutoff_date).execute()
                print(f"  > [Cleanup] å·²åˆªé™¤ {cutoff_date} ä¹‹å‰çš„èˆŠæ–°è")
                
                # 2. å¦‚æœè¶…é 100 æ¢ï¼Œåªä¿ç•™æœ€æ–°çš„ 100 æ¢
                result = supabase.table("ai_news").select("id", count="exact").execute()
                total_count = result.count if result.count else 0
                
                if total_count > 100:
                    # å–å¾—ç¬¬ 101 æ¢ä¹‹å¾Œçš„ ID ä¸¦åˆªé™¤
                    old_records = supabase.table("ai_news").select("id").order("created_at", desc=True).range(100, total_count).execute()
                    old_ids = [r['id'] for r in old_records.data] if old_records.data else []
                    
                    if old_ids:
                        for old_id in old_ids:
                            supabase.table("ai_news").delete().eq("id", old_id).execute()
                        print(f"  > [Cleanup] å·²åˆªé™¤è¶…é 100 æ¢é™åˆ¶çš„ {len(old_ids)} æ¢èˆŠæ–°è")
                        
            except Exception as cleanup_err:
                print(f"  > [Cleanup] æ¸…ç†æ™‚ç™¼ç”ŸéŒ¯èª¤: {cleanup_err}")
        else:
            print("\nâ˜ï¸ ä»Šæ—¥ç„¡æ–°å¢é«˜åƒ¹å€¼æ–°èéœ€åŒæ­¥è‡³ Supabaseã€‚")

if __name__ == "__main__":
    initialize_services()
    process_latest_news()
